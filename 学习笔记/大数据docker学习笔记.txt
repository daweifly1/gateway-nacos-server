k8s:
系统时centos7上 关闭防火墙 systemctl stop firewalld.service 关闭selinux vi /etc/selinux/comfig







keytool -genkey -v -keystore chendawei.keystore -alias chendawei.keystore -keyalg RSA -validity 20000


jarsigner -verbose -keystore chendawei.keystore -signedjar complete.apk app.apk chendawei.keystore




Ctrl + P + Q 退出容器保持后台运行（按住ctrl和P 再按Q），再用 docker ps 进行查看：  exit可以直接退出容器

dockerHub账号
chendawei320324

hdfs:
1.软件下载
http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz

sudo docker network create --subnet=192.168.0.0/16 staticnet

sudo docker run -it -p 1022:22 --name hadoop00 --net staticnet --ip 192.168.0.2 -v /c/Users/xcit/share/:/test ubuntu /bin/bash


iptables -t nat -A DOCKER -p tcp –dport 1022 -j DNAT –to-destination 192.168.0.2:22


cd /app

apt-get install vim -y

cd
vi .bash_profile  vi ~/.bashrc

export JAVA_HOME=/app/jdk8
export PATH=$JAVA_HOME/bin:$PATH
source .bash_profile

export JAVA_HOME=/app/jdk8
export PATH=$JAVA_HOME/bin:$PATH

export  JAVA_HOME=/app/jdk8
export  HADOOP_HOME=/app/hadoop260
export  HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export  HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export  HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib"

#export  HIVE_HOME=/opt/hive/apache-hive-2.1.1-bin
#export  HIVE_CONF_DIR=${HIVE_HOME}/conf
#export  SQOOP_HOME=/opt/sqoop/sqoop-1.4.6.bin__hadoop-2.0.4-alpha
export  HBASE_HOME=/app/hbase-1.2.0-cdh5.7.0
export  ZK_HOME=/app/zookeeper345
export  CLASS_PATH=.:${JAVA_HOME}/lib:${HIVE_HOME}/lib:$CLASS_PATH
export PATH=.:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${ZOOKEEPER_HOME}/bin:${HIVE_HOME}/bin:${SQOOP_HOME}/bin:${HBASE_HOME}:${ZK_HOME}/bin:$PATH




apt install net-tools
docker commit 3c9a chendawei/ubuntu
sudo docker run -it -p 1022:22 --name hadoop011 --net staticnet --ip 192.168.0.9 -v /c/Users/xcit/share/:/test chendawei/ubuntu /bin/bash

ssh 安装
apt-get install ssh
/etc/init.d/ssh start
配置ssh免密码登录






修改~/.bashrc文件。在文件末尾加入下面配置信息：

export JAVA_HOME=/app/jdk8
export PATH=$JAVA_HOME/bin:$PATH
export HADOOP_HOME=/app/hadoop260
export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#autorun
/usr/sbin/sshd

cd $HADOOP_HOME/

mkdir tmp
mkdir namenode
mkdir datanode

cd $HADOOP_CONFIG_HOME/
cp mapred-site.xml.template mapred-site.xml
nano hdfs-site.xml

1).core-site.xml配置

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
            <name>hadoop.tmp.dir</name>
            <value>/app/hadoop260/tmp</value>
            <description>A base for other temporary directories.</description>
    </property>

    <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
            <final>true</final>
            <description>fs.default.name</description>
    </property>
</configuration>

2).hdfs-site.xml配置

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
<name>dfs.replication</name>
<value>2</value>
<final>true</final>
<description>Default block replication.
The actual number of replications can be specified when the file is created.
The default is used if replication is not specified in create time.
</description>
</property>

<property>
<name>dfs.namenode.name.dir</name>
<value>/app/hadoop260/namenode</value>
<final>true</final>
</property>

<property>
<name>dfs.datanode.data.dir</name>
<value>/app/hadoop260/datanode</value>
<final>true</final>
</property>

<property>
<name>dfs.namenode.secondary.http-address</name>
<value>master:9001</value>
</property>
<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>
</configuration>




3).mapred-site.xml配置

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
<name>mapred.job.tracker</name>
<value>master:9001</value>
<description>The host and port that the MapReduce job tracker runs
at.  If "local", then jobs are run in-process as a single map
and reduce task.
</description>
</property>
</configuration>




<property> <!--mapreduce运用了yarn框架，设置name为yarn--> 
<name>mapreduce.framework.name</name> 
<value>yarn</value> 
</property> 
<property> <!--历史服务器,查看Mapreduce作业记录--> 
<name>mapreduce.jobhistory.address</name> 
<value>master:10020</value> 
</property> 
<property> 
<name>mapreduce.jobhistory.webapp.address</name> 
<value>master:19888</value> 
</property> 



<configuration>

　　　　<property> <!--NodeManager上运行的附属服务，用于运行mapreduce--> 
　　　　　　<name>yarn.nodemanager.aux-services</name> 
　　　　　　<value>mapreduce_shuffle</value> 
　　　　</property> 
　　　　<property> 
　　　　　　<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> 
　　　　　　<value>org.apache.hadoop.mapred.ShuffleHandler</value> 
　　　　</property> 
　　　　<property> <!--ResourceManager 对客户端暴露的地址--> 
　　　　　　<name>yarn.resourcemanager.address</name> 
　　　　　　<value>master:8032</value> 
　　　　</property> 
　　　　<property> <!--ResourceManager 对ApplicationMaster暴露的地址-->  
　　　　　　<name>yarn.resourcemanager.scheduler.address</name> 
　　　　　　<value>master:8030</value> 
　　　　</property> 
　　　　<property> <!--ResourceManager 对NodeManager暴露的地址--> 
　　　　　　<name>yarn.resourcemanager.resource-tracker.address</name>  
　　　　　　<value>master:8031</value> 
　　　　</property> 
　　　　<property> <!--ResourceManager 对管理员暴露的地址--> 
　　　　　　<name>yarn.resourcemanager.admin.address</name>   
　　　　　　<value>master:8033</value> 
　　　　</property> 
　　　　<property> <!--ResourceManager 对外web暴露的地址，可在浏览器查看-->   
　　　　　　<name>yarn.resourcemanager.webapp.address</name> 
　　　　　　<value>master:8088</value> 
　　　　</property> 
　　</configuration>

4)修改JAVA_HOME环境变量  hadoop-env.sh
export JAVA_HOME=/app/jdk8

5.格式化 namenode
hadoop namenode -format


5.生成访问密钥
root@01a36481e621:/# cd ~/
root@01a36481e621:~# ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
root@01a36481e621:~# cd .ssh
root@01a36481e621:~/.ssh# cat id_dsa.pub >> authorized_keys

6.保存镜像副本
这里我们将安装好Hadoop的镜像保存为一个副本。


docker commit -m "hadoop install" 01a36481e621 chendawei320324/ubuntu:hadoop


启动Docker容器

启动master容器
docker run -it -p 1022:22 -p 50070:50070 -p 16030:16030 --name master --net staticnet --ip 192.168.0.9 --add-host slave1:192.168.0.8 --add-host slave2:192.168.0.7 --add-host master:192.168.0.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash

启动slave1容器
docker run -it -p 1122:22 --name slave1 --net staticnet --ip 192.168.0.8 --add-host slave1:192.168.0.8 --add-host slave2:192.168.0.7 --add-host master:192.168.0.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash
启动slave2容器
docker run -it -p 1222:22 --name slave2 --net staticnet --ip 192.168.0.7 --add-host slave1:192.168.0.8 --add-host slave2:192.168.0.7 --add-host master:192.168.0.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash




配置hosts   vi  /etc/hosts

192.168.0.9	master
192.168.0.8	slave1
192.168.0.7	slave2

cd $HADOOP_CONFIG_HOME/
vi slaves 

slave1
slave2


在master节点上通过命令hdfs dfsadmin -report查看DataNode是否正常启动




$ docker ps // 查看所有正在运行容器
$ docker stop containerId // containerId 是容器的ID

$ docker ps -a // 查看所有容器
$ docker ps -a -q // 查看所有容器ID

$ docker stop $(docker ps -a -q) //  stop停止所有容器
$ docker  rm $(docker ps -a -q) //   remove删除所有容器


启动Docker容器

启动master容器
docker run -it -p 1022:22 -p 50070:50070 --name master --net staticnet --ip 192.168.99.9 --add-host slave1:192.168.99.8 --add-host slave2:192.168.99.7 --add-host master:192.168.99.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash

启动slave1容器
docker run -it -p 1122:22 --name slave1 --net staticnet --ip 192.168.99.8 --add-host slave1:192.168.99.8 --add-host slave2:192.168.99.7 --add-host master:192.168.99.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash

启动slave2容器
docker run -it -p 1222:22 --name slave2 --net staticnet --ip 192.168.99.7 --add-host slave1:192.168.99.8 --add-host slave2:192.168.99.7 --add-host master:192.168.99.9 -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash

配置文件hdfs-site.xml找到dfs.datanode.name.dir这个属性，或者dfs.data.dir具体看你用哪个设置的数据存储路径。

 

在master节点上执行start-all.sh命令，启动Hadoop。


启动Docker容器

docker run -it -p 50070:50070 -p 60010 --name master  -v /c/Users/xcit/share/:/test chendawei320324/ubuntu:hadoop /bin/bash

启动slave1容器
docker run -it -p 1122:22 --name slave1 -v /c/Users/xcit/share/:/test chendawei320324/ubuntu:hadoop /bin/bash

启动slave2容器
docker run -it -p 1222:22 --name slave2 -v /c/Users/xcit/share/:/test chendawei320324/ubuntu:hadoop /bin/bash

vi  /etc/hosts

172.17.0.2	master
172.17.0.3	slave1
172.17.0.4	slave2


#!/bin/bash
echo 172.17.0.2	master >> /etc/hosts
echo 172.17.0.3	slave1 >> /etc/hosts
echo 172.17.0.4	slave2 >> /etc/hosts
echo 172.17.0.2 regionserver1 >> /etc/hosts
echo 172.17.0.3 regionserver2 >> /etc/hosts


autopurge.snapRetainCount=500
autopurge.purgeInterval=24


server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888

core-site.xml

<property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoopha</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/bigdata/hadoop/tmp</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>slave1:2181,bigdata3:2181,bigdata4:2181,bigdata5:2181,bigdata6:2181</value>
    </property>
	
	
	



cd $HADOOP_CONFIG_HOME/

start-all.sh


docker run -it  -v /c/Users/xcit/share/:/test ubuntu:hadoop /bin/bash





http://www.ekgc.cn/news/courseCatalog?id=27881&json=1




MR wordcount 例子：

hadoop jar /test/wc.jar com.kgc.hadoop.mapreduce.WordCountApp /word.txt /output





hbase

hdfs dfsadmin -safemode leave


export  JAVA_HOME=/app/jdk8
export  HADOOP_HOME=/app/hadoop260
export  HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export  HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export  HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib"

#export  HIVE_HOME=/opt/hive/apache-hive-2.1.1-bin
#export  HIVE_CONF_DIR=${HIVE_HOME}/conf
#export  SQOOP_HOME=/opt/sqoop/sqoop-1.4.6.bin__hadoop-2.0.4-alpha
export  HBASE_HOME=/app/hbase-1.2.0-cdh5.7.0
export  ZK_HOME=/app/zookeeper345
export  CLASS_PATH=.:${JAVA_HOME}/lib:${HIVE_HOME}/lib:$CLASS_PATH
exportPATH=.:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${ZOOKEEPER_HOME}/bin:${HIVE_HOME}/bin:${SQOOP_HOME}/bin:${HBASE_HOME}:${ZK_HOME}/bin:$PATH

HADOOP_COMMON_LIB_NATIVE_DIR
